# cerebrium.toml
[cerebrium.deployment]
name = "image-classifier-prod"

# This flag is essential for custom Docker image deployment
#use_dockerfile = true 

[cerebrium.hardware]
# T4 is a good balance of performance and cost for inference
compute = "TURING_T4" 
memory = 8.0

[cerebrium.scaling]
# Scale to zero when idle to minimize costs on the free plan
min_replicas = 0 

[build]
# The 'include' key must list every file your application needs to run.
# Make sure "model.onnx" is present in this list.
include = [
    "model.py",
    "main.py",
    "model.onnx" # <-- This is the missing piece
]